# Aggressive Transformer Fusion – Technical Brief

## Data Pipeline & Inputs
- Lightning loads the Hierarchical Data Format version 5 (HDF5) dataset `data/h5/all_train_transformer_v2_optimized.h5` into Random Access Memory (RAM)-backed memmaps, so every Distributed Data Parallel (DDP) worker reads shared tensors instead of duplicating data (`src/data/shared_memory_datamodule_v2.py:135`–`src/data/shared_memory_datamodule_v2.py:260`). The loader detects transformer mode, materialises neighbour indices/distances, and writes them alongside tabular and Evolutionarily Scale Model (ESM) arrays.
- Each epoch starts with imbalance bookkeeping that reports ~4.30 M training samples with 2.55 % positives and updates the model’s `pos_weight` to 38.3 (`training_aggressive_2025-10-21_12-48-11.log:295`, `training_aggressive_2025-10-21_12-48-11.log:309`, `training_aggressive_2025-10-21_12-48-11.log:312`).
- Samples expose two feature types: 35 physicochemical descriptors (`configs/experiment/fusion_transformer_aggressive_oct17.yaml:36`) and 2 560-D residue embeddings from ESM2; k-nearest neighbour (k-NN) metadata (indices, distances, residue numbers) is stored for the top three neighbours (`configs/experiment/fusion_transformer_aggressive_oct17.yaml:75`–`configs/experiment/fusion_transformer_aggressive_oct17.yaml:78`, `src/data/knn_utils.py:150`–`src/data/knn_utils.py:198`).

## Model Architecture
- **Encoders.** Separate multi-layer perceptron (MLP) stacks embed tabular features, the central residue, and the neighbour context (`src/models/esm_tabular_module.py:320`–`src/models/esm_tabular_module.py:336`). Layer normalization plus dropout prepare ESM representations when transformer aggregation is active (`src/models/esm_tabular_module.py:294`–`src/models/esm_tabular_module.py:317`).
- **Neighbour attention.** The `NeighborAttentionEncoder` performs multi-head self-attention with optional learned distance bias and two transformer layers (`src/models/neighbor_attention_encoder.py:41`–`src/models/neighbor_attention_encoder.py:188`). The centre embedding is appended as an explicit neighbour so the encoder can learn to ignore or reuse it (`src/models/esm_tabular_module.py:521`–`src/models/esm_tabular_module.py:540`).
- **Per-sample gating.** A lightweight MLP predicts α in `[0,1]`, blending the untouched centre embedding with the attention-aggregated neighbour context; α is temperature-scaled, optionally dropout-perturbed toward `gate_dropout_target`, and the blended context is re-normalised (`src/models/esm_tabular_module.py:543`–`src/models/esm_tabular_module.py:569`).
- **Fusion head.** After aggregation the model concatenates tabular, centre, and neighbour representations and feeds them into a two-layer classifier with batch normalization and dropout (`src/models/esm_tabular_module.py:377`–`src/models/esm_tabular_module.py:439`). An auxiliary neighbour-only head can supply distance-weighted binary cross-entropy (BCE) loss to keep gradients flowing through attention (`src/models/esm_tabular_module.py:412`–`src/models/esm_tabular_module.py:427`).
- **Losses & metrics.** Training defaults to BCE with logits plus optional focal and asymmetric loss (ASL) alternatives; the imbalance callback seeds the output bias from the empirical positive prior and logs a rich set of validation/test metrics—area under the receiver operating characteristic curve (AUROC), area under the precision-recall curve (AUPRC), F1-score (F1), and Intersection over Union (IoU)—each epoch (`src/models/esm_tabular_module.py:445`–`src/models/esm_tabular_module.py:483`, `training_aggressive_2025-10-21_12-48-11.log:312`–`training_aggressive_2025-10-21_12-48-11.log:317`).

## Transformer-Based k-NN Aggregation
1. The datamodule streams the centre residue embedding plus neighbour indices/distances; when transformer mode is active, embeddings are materialised on the fly with masking for padded neighbours (`src/models/esm_tabular_module.py:487`–`src/models/esm_tabular_module.py:520`).
2. Neighbour embeddings and distances pass through the attention encoder, which injects distance-aware bias per head/layer and returns both the contextualised vector and optional entropy diagnostics used for logging (`src/models/neighbor_attention_encoder.py:41`–`src/models/neighbor_attention_encoder.py:188`).
3. The gating MLP blends centre vs. neighbour context and tracks α, dropout rate, and entropy for regularisation. Schedules anneal `gate_penalty`, `gate_dropout`, and the neighbour scaling factor over the first 15 epochs in the aggressive recipe (`configs/experiment/fusion_transformer_aggressive_oct17.yaml:55`–`configs/experiment/fusion_transformer_aggressive_oct17.yaml:73`).
4. During validation the module caches α-distributions, context norms, and attention entropies so diagnostics can be exported, which is why logs show per-epoch summaries such as gate weight trends and entropy collapse (`training_aggressive_2025-10-17_16-31-52.log:508`, `training_aggressive_2025-10-17_16-31-52.log:534`).

## Aggressive Training Regime
- The launcher presets batch size 640 across all Graphics Processing Units (GPUs), the One-Cycle Learning Rate (OneCycleLR) policy with `max_lr=5e-4`, modality dropout of 0.1, and the gate/aux schedules described above (`configs/experiment/fusion_transformer_aggressive_oct17.yaml:20`–`configs/experiment/fusion_transformer_aggressive_oct17.yaml:97`, `launch_aggressive_training.sh:14`–`launch_aggressive_training.sh:33`).
- Typical learning dynamics: the 20 Oct run reached its best validation AUPRC of **0.3074** at epoch 14 with α≈0.62, entropy ≈0.54, and F1 ≈0.37 before drifting downward as the learning rate (LR) annealed (`training_aggressive_2025-10-20_17-19-10.log:561`). Earlier on 17 Oct the annealed gate held α≈0.55 through 30 epochs while maintaining validation AUPRC ≈0.292 and more stable entropy ≈0.59 (`training_aggressive_2025-10-17_16-31-52.log:508`, `training_aggressive_2025-10-17_16-31-52.log:534`).
- Test-time checkpoints corresponding to these aggressive runs land between 0.40–0.44 AUPRC, with the best snapshot (epoch 14) logging 0.424 on the hold-out set (`MODEL_CHANGELOG_OCT2025.md:22`).

## Stochastic Weight Averaging (SWA) Finetuning Path
- `launch_aggressive_swa.sh` resumes the aggressive experiment, disables early stopping, and attaches Lightning’s SWA callback starting at epoch 20 with a 10-epoch cosine anneal and averaged LR of 2e-4 (`launch_aggressive_swa.sh:15`–`launch_aggressive_swa.sh:45`).
- When SWA was launched from the epoch‑14 checkpoint on 20 Oct, the averaged model briefly matched 0.301 AUPRC but settled at 0.274 as α drifted toward 0.67 and attention entropy collapsed (`training_aggressive_swa_2025-10-20_20-37-58.log:525`).
- The longer 17 Oct SWA run (60 epochs) activated averaging between epochs 20–30, peaked at **0.3133** AUPRC with α≈0.55 and entropy ≈0.28, and preserved a strong test AUPRC of 0.414 even though validation later regressed (`training_aggressive_swa_2025-10-17_23-30-27.log:502`–`training_aggressive_swa_2025-10-17_23-30-27.log:555`, `MODEL_CHANGELOG_OCT2025.md:21`).

## Key Metrics Snapshot
| Regime | Best Val AUPRC | Best Val IoU | Test AUPRC | Test IoU | Notes |
|-------|----------------|--------------|------------|----------|-------|
| Aggressive (Oct 20) | 0.3074 | 0.2217 | 0.424 | 0.278 | α≈0.62, entropy 0.54 at peak (`training_aggressive_2025-10-20_17-19-10.log:561`, `training_aggressive_2025-10-20_17-19-10.log:784`, `training_aggressive_2025-10-20_17-19-10.log:650`) |
| Aggressive + SWA (Oct 17) | 0.3133 | 0.2191 | 0.414 | 0.295 | Averaging epoch 20–30, α≈0.55, entropy 0.28 (`training_aggressive_swa_2025-10-17_23-30-27.log:555`, `training_aggressive_swa_2025-10-17_23-30-27.log:803`, `training_aggressive_swa_2025-10-17_23-30-27.log:669`) |
| Aggressive + SWA (Oct 20) | 0.3012 | 0.2186 | 0.424 | 0.278 | α drifted >0.66 post-averaging (`training_aggressive_swa_2025-10-20_20-37-58.log:525`, `training_aggressive_swa_2025-10-20_20-37-58.log:750`, `training_aggressive_swa_2025-10-20_20-37-58.log:616`) |

## Current Observations & Next Steps
- The gate penalty and dropout schedules successfully hold α near the 0.55 target early in training, but averaging phases can release that pressure, causing the blend to favour centres again and erode recall; monitoring α post-SWA is critical.
- Validation curves consistently peak before LR fully anneals, signalling that either stronger regularisation late in training or snapshot ensembling may outperform pure SWA—especially because the best single checkpoint already matches the averaged test score.
- Upcoming experiments can focus on delaying SWA start, shortening the averaging window, or adding a contrastive auxiliary term to maintain neighbour entropy as suggested in the changelog (`MODEL_CHANGELOG_OCT2025.md:42`–`MODEL_CHANGELOG_OCT2025.md:46`).
