# @package _global_

# example hyperparameter optimization of some experiment with Optuna:
# python train.py -m hparams_search=binding_site_optuna experiment=binding_site

defaults:
  - override /hydra/sweeper: optuna

# choose metric which will be optimized by Optuna
# make sure this is the correct name of some metric logged in lightning module!
optimized_metric: "val/loss"

# here we define Optuna hyperparameter search
# it optimizes for value returned from function with @hydra.main decorator
# docs: https://hydra.cc/docs/next/plugins/optuna_sweeper
hydra:
  mode: MULTIRUN # mandatory mode for optimization
  sweeper:
    _target_: hydra._internal.utils.create_config_search_path
    search_space:
      model.optimizer.lr:
        type: float
        low: 0.0001
        high: 0.1
        log: true
      model.n_d:
        type: categorical
        choices: [64, 128, 256]
      model.n_a:
        type: categorical
        choices: [64, 128, 256]
      model.n_steps:
        type: int
        low: 3
        high: 10
      model.gamma:
        type: float
        low: 1.0
        high: 2.0
      data.batch_size:
        type: categorical
        choices: [32, 64, 128]
      data.sampling_strategy:
        type: categorical
        choices: ["none", "oversample", "undersample", "combined"]
    direction: minimize
    n_trials: 100
    n_jobs: 1

    # for optuna to work, we need to select which config will be used by default
    # schematic syntax: config_group=option_name
    # group 'config_group' needs to be added to config.yaml as default
    # then we can override it with 'config_group=option_name'
    # here we select: experiment=binding_site, and model=tabnet_binding_site
    experiment: binding_site
    model: tabnet_binding_site
