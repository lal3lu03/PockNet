# @package _global_

# XGBoost hyperparameter optimization with Optuna:
# python train.py -m hparams_search=xgboost_binding_site_optuna experiment=binding_site model=xgboost_binding_site

defaults:
  - override /hydra/sweeper: optuna

# choose metric which will be optimized by Optuna
optimized_metric: "val/loss"

hydra:
  mode: MULTIRUN # mandatory mode for optimization
  sweeper:
    _target_: hydra._internal.utils.create_config_search_path
    search_space:
      model.learning_rate:
        type: float
        low: 0.01
        high: 0.3
        log: true
      model.max_depth:
        type: int
        low: 3
        high: 10
      model.n_estimators:
        type: categorical
        choices: [50, 100, 200, 300]
      model.subsample:
        type: float
        low: 0.6
        high: 1.0
      model.colsample_bytree:
        type: float
        low: 0.6
        high: 1.0
      model.gamma:
        type: float
        low: 0.0
        high: 5.0
      model.reg_alpha:
        type: float
        low: 0.0
        high: 1.0
      model.reg_lambda:
        type: float
        low: 0.0
        high: 2.0
      data.batch_size:
        type: categorical
        choices: [32, 64, 128]
      data.sampling_strategy:
        type: categorical
        choices: ["none", "oversample", "undersample", "combined"]
    direction: minimize
    n_trials: 100
    n_jobs: 1

    experiment: binding_site
    model: xgboost_binding_site
