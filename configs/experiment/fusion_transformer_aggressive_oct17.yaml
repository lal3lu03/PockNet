# @package _global_

# October 17th 2025 configuration — reproduces the first 0.31+ AUPRC baseline.

defaults:
  - override /data: h5_transformer_aggregation
  - override /model: esm_tabular_transformer
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: wandb

task_name: "fusion_transformer_aggressive_oct17"
tags: ["esm2", "tabular", "fusion", "transformer", "attention", "knn", "aggressive", "oct17_baseline"]

seed: 42

train: true
test: true

trainer:
  min_epochs: 5
  max_epochs: 80
  gradient_clip_val: 0.5
  devices: -1  # use all visible GPUs (4 on current node)
  strategy: "ddp_find_unused_parameters_true"
  precision: "32-true"
  sync_batchnorm: true
  log_every_n_steps: 50
  accumulate_grad_batches: 1
  val_check_interval: 1.0
  check_val_every_n_epoch: 1

model:
  _target_: src.models.esm_tabular_module.EsmTabularModule

  tabular_dim: 35
  esm_dim: 2560
  hidden_dims: [1024, 512, 256]
  dropout: 0.2

  fusion_method: "concat"
  fusion_dim: 512
  modality_dropout_p: 0.1

  aggregation_mode: "transformer"

  neighbor_attention_heads: 6
  neighbor_attention_layers: 2
  neighbor_attention_dim_ff: 1280
  neighbor_attention_dropout: 0.2
  distance_bias_type: "learned"
  attention_pooling: "mean"
  neighbor_attention_proj_dim: 2400

  gate_penalty_weight: 0.15
  gate_penalty_target: 0.6
  gate_temperature: 1.5
  gate_dropout_p: 0.25
  gate_dropout_target: 0.55
  neighbor_context_scale: 0.5
  gate_penalty_start: 0.15
  gate_penalty_end: 0.05
  gate_dropout_start: 0.25
  gate_dropout_end: 0.05
  neighbor_context_scale_start: 0.5
  neighbor_context_scale_end: 1.0
  gate_schedule_epochs: 15

  aux_neighbor_head_weight: 0.10
  aux_neighbor_head_weight_start: 0.10
  aux_neighbor_head_weight_end: 0.02
  aux_neighbor_head_schedule_epochs: 12
  aux_distance_temperature: 6.0

  k_res_neighbors: 3
  neighbor_weighting: "softmax"
  neighbor_temp: 2.0

  num_classes: 1
  loss_type: "bce"
  initial_pos_prior: 0.0246

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 0.0001
    weight_decay: 0.001

  scheduler:
    _target_: torch.optim.lr_scheduler.OneCycleLR
    _partial_: true
    max_lr: 0.0005
    pct_start: 0.2
    anneal_strategy: "cos"
    div_factor: 25.0
    final_div_factor: 10000.0

logger:
  wandb:
    project: "fusion_pocknet_thesis"
    name: "${hydra:job.num}_transformer_aggressive-oct17_${now:%H%M%S}"
    tags:
      - transformer
      - aggressive
      - attention
      - knn3
      - oct17
      - reproduction
    group: "transformer_aggressive_repro_${now:%Y%m%d}"
    save_code: true
    log_model: false
    notes: "Reproducing Oct17 aggressive transformer baseline (penalty 0.15→0.05, context 0.5→1.0)."

callbacks:
  imbalance_setup:
    _target_: src.callbacks.imbalance_setup.ImbalanceSetup

  model_checkpoint:
    monitor: "val/auprc"
    mode: "max"
    save_top_k: 3
    save_last: true
    filename: "epoch_{epoch:02d}-val_auprc_{val/auprc:.4f}"
    dirpath: "${hydra:run.dir}/checkpoints"
    verbose: true

  early_stopping:
    monitor: "val/auprc"
    mode: "max"
    patience: 15
    min_delta: 0.001
    verbose: true

  learning_rate_monitor:
    logging_interval: "step"
    log_momentum: true

hydra:
  job:
    chdir: true
