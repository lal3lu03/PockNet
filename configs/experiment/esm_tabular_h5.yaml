# @package _global_

# to execute this experiment run:
# python train.py experiment=esm_tabular_h5

defaults:
  - override /data: h5_esm
  - override /model: esm_tabular
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: wandb

# all parameters below will be merged with parameters from default configurations set above
# this allows you to overwrite only specified parameters

task_name: "train_esm_tabular_h5"
tags: ["esm2", "tabular", "h5", "residue-specific", "attention-fusion"]

seed: 42

# Training and test settings
train: True
test: True

# Trainer configurations  
trainer:
  min_epochs: 5
  max_epochs: 50
  gradient_clip_val: 1.0
  devices: [0, 1, 2, 3]  # Use all 4 GPUs
  strategy: "ddp"
  precision: "16-mixed"
  log_every_n_steps: 100
  val_check_interval: 0.25

# Data configurations
data:
  batch_size: 512  # Optimize for GPU memory
  num_workers: 16  # Use many CPU cores
  pin_memory: true
  persistent_workers: true

# Model configurations
model:
  fusion_method: "attention"
  hidden_dims: [1024, 512, 256]
  dropout: 0.2
  optimizer:
    lr: 0.001
    weight_decay: 0.01

# Callbacks
callbacks:
  model_checkpoint:
    monitor: "val/f1"
    mode: "max"
    save_top_k: 3
    filename: "epoch_{epoch:02d}-f1_{val/f1:.4f}"
  
  early_stopping:
    monitor: "val/f1"
    mode: "max"
    patience: 15
    min_delta: 0.001

# Logger
logger:
  wandb:
    project: "PockNet_ESM"
    group: "esm_tabular_experiments"
    name: "ESM2_Tabular_H5_AttentionFusion"
    tags: ${tags}
