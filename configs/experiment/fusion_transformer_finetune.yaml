# @package _global_

# Targeted fine-tuning configuration that focuses on boundary samples.

defaults:
  - override /data: h5_transformer_aggregation
  - override /model: esm_tabular_transformer
  - override /callbacks: default
  - override /trainer: gpu
  - override /logger: wandb

task_name: "fusion_transformer_finetune"
tags: ["esm2", "tabular", "fusion", "transformer", "finetune", "boundary"]

seed: 42

train: true
test: true

trainer:
  min_epochs: 1
  max_epochs: 24
  devices: -1
  precision: "32-true"
  strategy: "ddp_find_unused_parameters_true"
  gradient_clip_val: 0.5
  accumulate_grad_batches: 1
  log_every_n_steps: 25
  check_val_every_n_epoch: 1
  val_check_interval: 1.0

model:
  _target_: src.models.esm_tabular_module.EsmTabularModule

  tabular_dim: 35
  esm_dim: 2560
  hidden_dims: [1024, 512, 256]
  dropout: 0.2

  fusion_method: "concat"
  fusion_dim: 512
  modality_dropout_p: 0.0

  aggregation_mode: "transformer"

  neighbor_attention_heads: 6
  neighbor_attention_layers: 2
  neighbor_attention_dim_ff: 1280
  neighbor_attention_dropout: 0.15
  distance_bias_type: "learned"
  attention_pooling: "mean"
  neighbor_attention_proj_dim: 2400

  gate_penalty_weight: 0.10
  gate_penalty_target: 0.70
  gate_temperature: 1.5
  gate_dropout_p: 0.15
  gate_dropout_target: 0.55
  neighbor_context_scale: 0.55
  gate_schedule_epochs: 4
  gate_penalty_start: 0.10
  gate_penalty_end: 0.08
  gate_dropout_start: 0.15
  gate_dropout_end: 0.10
  neighbor_context_scale_start: 0.50
  neighbor_context_scale_end: 0.65

  aux_neighbor_head_weight: 0.02
  aux_neighbor_head_weight_start: 0.02
  aux_neighbor_head_weight_end: 0.0
  aux_neighbor_head_schedule_epochs: 4
  aux_distance_temperature: 6.0

  k_res_neighbors: 3
  neighbor_weighting: "softmax"
  neighbor_temp: 2.0

  num_classes: 1
  loss_type: "bce"
  initial_pos_prior: 0.0246

  freeze_center_encoder: true
  freeze_context_encoder: true
  freeze_tabular_encoder: false
  freeze_neighbor_encoder: false

  optimizer:
    _target_: torch.optim.AdamW
    _partial_: true
    lr: 1.0e-5
    weight_decay: 5.0e-4

  scheduler:
    _target_: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
    _partial_: true
    T_0: 200
    T_mult: 2
    eta_min: 1.0e-6

  scheduler_interval: step
  scheduler_frequency: 1

data:
  batch_size: 256
  hard_positive_indices_path: null
  hard_positive_repeat: 1
  train_indices_override_path: null
  train_override_shuffle_seed: 2025

callbacks:
  imbalance_setup:
    _target_: src.callbacks.imbalance_setup.ImbalanceSetup
    override_pos_weight: true
    override_bias: true
    manual_pos_weight: 8.0
  early_stopping:
    monitor: "val/auprc"
    patience: 10
    min_delta: 0.0005
    mode: "max"
  model_checkpoint:
    monitor: "val/auprc"
    mode: "max"
    save_top_k: 3
    filename: "finetune_epoch_{epoch:02d}-val_auprc_{val/auprc:.4f}"

logger:
  wandb:
    project: "fusion_pocknet_thesis"
    name: "${hydra:job.num}_finetune_boundary-${now:%H%M%S}"
    tags:
      - finetune
      - boundary
      - transformer
      - ${model.fusion_method}
      - ${model.loss_type}
      - curated
    group: "finetune_boundary_${now:%Y%m%d}"
    save_code: true
    log_model: false
    notes: "Boundary-focused fine-tuning with curated train_indices override."

hydra:
  job:
    chdir: true
