#!/usr/bin/env python3
"""
Test script for the end-to-end post-processing pipeline

This script validates the complete pipeline functionality using a small subset
of data to ensure everything works before running on the full dataset.
"""

import sys
import os
import logging
from pathlib import Path
import numpy as np

# Add project root to Python path
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


def test_single_model_pipeline():
    """Test single model inference pipeline"""
    logger.info("Testing single model pipeline...")
    
    try:
        from post_processing.inference import ModelInference
        
        # Use the same checkpoint and H5 file from our integration test
        checkpoint_path = "/system/user/studentwork/hageneder/MSC/Practical_work/PockNet/logs/fusion_all_train_complete/runs/2025-08-29_14-35-52/checkpoints/last.ckpt"
        h5_path = "/system/user/studentwork/hageneder/MSC/Practical_work/PockNet/data/h5/pocknet_with_esm2_3b.h5"
        
        # Initialize inference
        model_inference = ModelInference(checkpoint_path=checkpoint_path)
        
        # Run on a small subset
        predictions = model_inference.predict_from_h5(
            h5_file=h5_path,
            protein_ids=None,  # Test with all proteins but limit below
            batch_size=512
        )
        
        # Limit to first 5 for testing
        limited_predictions = dict(list(predictions.items())[:5])
        
        logger.info(f"‚úÖ Single model test passed: {len(limited_predictions)} proteins processed")
        
        # Validate predictions
        for protein_id, preds in list(limited_predictions.items())[:3]:
            logger.info(f"  {protein_id}: {len(preds)} residues, range [{preds.min():.4f}, {preds.max():.4f}]")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Single model test failed: {e}")
        return False


def test_ensemble_pipeline():
    """Test ensemble inference pipeline"""
    logger.info("Testing ensemble pipeline...")
    
    try:
        from post_processing.inference import MultiSeedInference
        
        # Use multiple checkpoints (just use same one multiple times for testing)
        base_checkpoint = "/system/user/studentwork/hageneder/MSC/Practical_work/PockNet/logs/fusion_all_train_complete/runs/2025-08-29_14-35-52/checkpoints/last.ckpt"
        checkpoint_paths = [base_checkpoint, base_checkpoint]  # Duplicate for testing
        h5_path = "/system/user/studentwork/hageneder/MSC/Practical_work/PockNet/data/h5/pocknet_with_esm2_3b.h5"
        
        # Initialize ensemble inference
        ensemble_inference = MultiSeedInference(checkpoint_paths=checkpoint_paths)
        
        # Run on a small subset
        predictions = ensemble_inference.predict_ensemble_from_h5(
            h5_file=h5_path,
            protein_ids=None  # Process all but limit below
        )
        
        # Limit to first 3 for testing
        limited_predictions = dict(list(predictions.items())[:3])
        
        logger.info(f"‚úÖ Ensemble test passed: {len(limited_predictions)} proteins processed")
        
        # Validate ensemble predictions
        for protein_id, preds in list(limited_predictions.items())[:2]:
            logger.info(f"  {protein_id}: {len(preds)} residues, range [{preds.min():.4f}, {preds.max():.4f}]")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Ensemble test failed: {e}")
        return False


def test_evaluation():
    """Test evaluation functionality"""
    logger.info("Testing evaluation functionality...")
    
    try:
        import h5py
        from sklearn.metrics import roc_auc_score, average_precision_score
        
        h5_path = "/system/user/studentwork/hageneder/MSC/Practical_work/PockNet/data/h5/pocknet_with_esm2_3b.h5"
        
        # Load a small sample of ground truth data
        with h5py.File(h5_path, 'r') as f:
            protein_keys = f['protein_keys'][:1000].astype(str)  # First 1000 samples
            labels = f['labels'][:1000]
        
        # Create dummy predictions
        predictions = np.random.rand(len(labels))
        
        # Test evaluation metrics
        if len(np.unique(labels)) > 1:
            auc = roc_auc_score(labels, predictions)
            auprc = average_precision_score(labels, predictions)
            
            logger.info(f"‚úÖ Evaluation test passed: AUC={auc:.4f}, AUPRC={auprc:.4f}")
            return True
        else:
            logger.warning("‚ö†Ô∏è Cannot test evaluation: no positive labels in sample")
            return True
        
    except Exception as e:
        logger.error(f"‚ùå Evaluation test failed: {e}")
        return False


def test_configuration_loading():
    """Test configuration loading"""
    logger.info("Testing configuration loading...")
    
    try:
        from omegaconf import OmegaConf
        
        config_path = "configs/post_processing/thesis.yaml"
        
        if Path(config_path).exists():
            # Load with interpolation resolution
            OmegaConf.register_new_resolver("oc.env", lambda x, default=None: os.environ.get(x, default))
            
            # Add root_dir to make interpolation work
            config = OmegaConf.load(config_path)
            config.paths = {"root_dir": str(Path(".").resolve())}
            config = OmegaConf.merge(config, {"paths": {"root_dir": str(Path(".").resolve())}})
            
            # Resolve interpolations
            config = OmegaConf.to_container(config, resolve=True)
            config = OmegaConf.create(config)
            
            # Validate required fields
            assert 'model' in config
            assert 'data' in config
            assert 'ensemble' in config
            assert 'checkpoint_paths' in config.model
            
            logger.info("‚úÖ Configuration loading test passed")
            logger.info(f"  Model checkpoints: {len(config.model.checkpoint_paths)} found")
            logger.info(f"  H5 data file: {config.data.h5_file}")
            logger.info(f"  Ensemble method: {config.ensemble.method}")
            
            return True
        else:
            logger.error(f"‚ùå Configuration file not found: {config_path}")
            return False
        
    except Exception as e:
        logger.error(f"‚ùå Configuration loading test failed: {e}")
        return False


def main():
    """Run all pipeline tests"""
    logger.info("=" * 60)
    logger.info("TESTING POCKNET POST-PROCESSING PIPELINE")
    logger.info("=" * 60)
    
    tests = [
        ("Configuration Loading", test_configuration_loading),
        ("Single Model Pipeline", test_single_model_pipeline),
        ("Ensemble Pipeline", test_ensemble_pipeline),
        ("Evaluation", test_evaluation),
    ]
    
    passed = 0
    total = len(tests)
    
    for test_name, test_func in tests:
        logger.info(f"\n--- {test_name} ---")
        try:
            if test_func():
                passed += 1
            else:
                logger.error(f"Test failed: {test_name}")
        except Exception as e:
            logger.error(f"Test crashed: {test_name} - {e}")
    
    logger.info("\n" + "=" * 60)
    logger.info("TEST SUMMARY")
    logger.info("=" * 60)
    logger.info(f"Passed: {passed}/{total}")
    
    if passed == total:
        logger.info("üéâ All tests passed! Pipeline is ready for production use.")
        return 0
    else:
        logger.error("‚ùå Some tests failed. Please fix issues before using pipeline.")
        return 1


if __name__ == "__main__":
    exit_code = main()
    sys.exit(exit_code)