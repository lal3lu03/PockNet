# Model Change Log – October 2025

This document tracks the incremental changes applied to the transformer-based k-NN aggregation model and the resulting validation/test metrics. All runs use the dataset `data/h5/all_train_transformer_v2_optimized.h5` with k=3 neighbours.

| Date & Run | Key Changes | Files Touched | Val AUPRC | Test AUPRC / Notes |
|------------|-------------|---------------|-----------|--------------------|
| 2025‑10‑13 – `training_stabilized_2025-10-13_20-24-12.log` | FP32 training, OneCycle max LR 1e-3, gradient clip 0.5, dropout 0.2 (baseline transformer v1). | `configs/experiment/fusion_transformer_complete.yaml` | 0.243 | 0.347 |
| 2025‑10‑14 – `training_aggressive_2025-10-14_17-32-16.log` | Added learned residual gate, distance clamp to 30Å, batch 512/accum 2, neighbour attention 10h×FF1536. | `src/models/esm_tabular_module.py`, `configs/data/h5_transformer_aggregation.yaml`, `configs/experiment/fusion_transformer_aggressive.yaml` | **0.300** | **0.423** |
| 2025‑10‑15 – (in-progress) | Switched to per-sample gate MLP, centre+context encoders, enhanced neighbour logging. Initial attempts failed during training due to incorrect dropout handling (null tensor). | `src/models/esm_tabular_module.py` | – | Run aborted (`TypeError: zeros_like`), fixed in subsequent commit. |
| 2025‑10‑15 – `training_aggressive_2025-10-15_14-46-15.log` | Separate centre/context encoders, concatenate `[centre, blended]`, extended logging (alpha stats, neighbour norms). | `src/models/esm_tabular_module.py`, `configs/experiment/fusion_transformer_aggressive.yaml` | **0.308** | 0.417 (gate still high at ~0.98). |
| 2025‑10‑15 – `training_aggressive_2025-10-15_20-23-58.log` | Gate clamped to [0.3, 0.7], added α min/max logging. | `src/models/esm_tabular_module.py` | 0.188 | 0.409 (gate centred ~0.70; val AUPRC dropped, test similar). |
| 2025‑10‑16 – `training_aggressive_2025-10-16_09-04-10.log` | Soft penalty (`+0.02·|ᾱ-0.6|`), alpha logging. Penalty insufficient—α returned to ≈0.98. | `src/models/esm_tabular_module.py` | 0.269 | 0.417 |
| 2025‑10‑16 – `training_aggressive_2025-10-16_14-11-48.log` | 6-head attention with 2400-D projection, batch 640 (no grad accumulation), added attention-entropy logging; gate penalty unchanged. | `src/models/neighbor_attention_encoder.py`, `src/models/esm_tabular_module.py`, `configs/model/esm_tabular_transformer.yaml`, `configs/data/h5_transformer_aggregation.yaml`, `configs/experiment/fusion_transformer_aggressive.yaml`, `launch_aggressive_training.sh` | 0.301 | 0.407 (α ≈ 1.0 despite penalty; neighbour context norm >> centre). |
| 2025‑10‑16 – `training_aggressive_2025-10-16_16-49-24.log` | Stronger gate penalty (0.15), temperature 1.5, 25% gate dropout, normalized neighbour context. α settled near 0.55, attention entropy rose, context norm matched centre. | `src/models/esm_tabular_module.py`, `configs/model/esm_tabular_transformer.yaml`, `configs/experiment/fusion_transformer_aggressive.yaml`, `launch_aggressive_training.sh` | 0.298 | 0.428 (validation peaked early; late epochs drifted down when dropout remained high). |
| 2025‑10‑16 – `training_aggressive_2025-10-16_21-09-46.log` | Introduced gate penalty/dropout/context-scale annealing (0.15→0.05, 25%→5%, 0.5→1.0 across 15 epochs). α held ~0.55; context scale ramped to 1.0; val AUPRC still peaked ≈0.291 and collapsed late. | `src/models/esm_tabular_module.py`, `configs/model/esm_tabular_transformer.yaml`, `configs/experiment/fusion_transformer_aggressive.yaml`, `launch_aggressive_training.sh` | 0.291 | 0.433 (final val degraded once LR small, but test AUPRC remained 0.433). |
| 2025‑10‑17 – `training_aggressive_2025-10-17_12-48-47.log` | Added neighbour-only auxiliary head (weight 0.1) and switched trainer strategy to plain DDP. Run aborted immediately because DDP detected unused parameters; reverted to `ddp_find_unused_parameters_true`. | `src/models/esm_tabular_module.py`, `configs/model/esm_tabular_transformer.yaml`, `configs/experiment/fusion_transformer_aggressive.yaml`, `launch_aggressive_training.sh` | – | Run failed (DDP unused-parameter check). |
| 2025‑10‑17 – `training_aggressive_2025-10-17_12-51-53.log` | Auxiliary neighbour head active (weight 0.1) with scheduled gate regularisation; DDP set back to find unused parameters. α≈0.545, aux loss ≈0.32, attention entropy dropped from ~0.54→0.27. Val AUPRC peaked at 0.294 (epoch 2) and finished at 0.267; test AUPRC 0.391. | `src/models/esm_tabular_module.py`, `configs/experiment/fusion_transformer_aggressive.yaml`, `launch_aggressive_training.sh` | 0.294 | 0.391 (aux head improved stability but late validation still trends down). |
| 2025‑10‑17 – `training_aggressive_2025-10-17_16-31-52.log` | Aux head now distance-weighted and annealed (0.10→0.02); gate schedule unchanged. Validation tracked best 0.286 early but held 0.292 at epoch 30; test AUPRC rose to 0.444. Attention entropy recovered (~0.59) and aux loss shrank to 0.058. | `src/models/esm_tabular_module.py`, `configs/model/esm_tabular_transformer.yaml`, `configs/experiment/fusion_transformer_aggressive.yaml`, `launch_aggressive_training.sh` | 0.292 | 0.444 (late validation stabilised; still shy of the 0.30 target). |
| 2025‑10‑17 – `training_aggressive_swa_2025-10-17_20-51-57.log` | First SWA attempt (max_epochs=40) – early stopping halted training at epoch 17 before SWA kicked in; val AUPRC best 0.297 (epoch 2), final 0.285. | `launch_aggressive_swa.sh` | 0.297 | 0.285 (same as non-SWA baseline; SWA phase never started). |
| 2025‑10‑18 – `training_aggressive_2025-10-18_12-04-17.log` | Centre-biased gate (penalty target 0.8, context scale 0.4→0.7), gate temperature 2.0, aux head annealed 0.07→0.015. α settled ≈0.82, attention entropy <0.30 after epoch 8; val plateaued <0.30 and slid once LR annealed. | `configs/experiment/fusion_transformer_aggressive.yaml` | 0.293 | 0.398 (final val 0.267; precision ↑ but recall and entropy dipped late). |
| 2025‑10‑18 – `training_aggressive_swa_2025-10-17_23-30-27.log` | Relaunched SWA (60 epochs, patience stretched) from epoch‑02 checkpoint. SWA active epoch 20→30 with cosine window; aux head annealed. Val AUPRC peaked at **0.313** (during SWA) but ended 0.280; test AUPRC 0.414. Attention entropy settled ~0.30; aux loss ≈0.053. | `launch_aggressive_swa.sh`, `src/models/esm_tabular_module.py`, `configs/experiment/fusion_transformer_aggressive.yaml` | **0.313** | 0.414 (SWA delivered modest test gain yet val still below 0.30 target). |
| 2025‑10‑20 – `training_aggressive_2025-10-20_17-19-10.log` | Gate schedule refined (penalty 0.16→0.10, dropout 0.25→0.15 with 0.65 target, context scale 0.45→0.55), console summaries streamlined with IoU/threshold logging. α hovered ≈0.58, entropy ≈0.45; val hit 0.307 at epoch 14 before tapering, test AUPRC 0.424. | `configs/experiment/fusion_transformer_aggressive.yaml`, `launch_aggressive_training.sh`, `src/models/esm_tabular_module.py` | **0.307** | 0.424 (final val 0.280; checkpoint `epoch_14-val_auprc_0.3074.ckpt` kept for SWA). |
| 2025‑10‑20 – `training_aggressive_swa_2025-10-20_20-37-58.log` | SWA resumed from epoch‑14 checkpoint with 10-epoch window (start=20). Averaging tracked the plateau briefly (val AUPRC 0.301) but drifted down to 0.274 as α climbed >0.66; test AUPRC remained 0.424. | `launch_aggressive_swa.sh`, `src/models/esm_tabular_module.py` | 0.301 | 0.424 (averaged model underperforms snapshot; consider later SWA start & shorter window). |
| 2025‑10‑21 – `training_aggressive_2025-10-21_12-48-11.log` | Reproduced Oct‑17 config on all 4 GPUs (devices=-1, DDP find unused). Hit val AUPRC **0.313** at epoch 09 (checkpoint `epoch_09-val_auprc_0.3132.ckpt`); validation collapsed after gate penalty finished annealing, finishing at 0.138. Test AUPRC 0.437. | `configs/experiment/fusion_transformer_aggressive_oct17.yaml`, `launch_aggressive_training.sh` | **0.313** | 0.437 (best checkpoint recaptured; use for SWA reproduction). |
| 2025‑10‑21 – `training_aggressive_swa_2025-10-21_15-16-28.log` | SWA rerun from epoch 09 checkpoint (start=20, window=10). Val AUPRC peaked at 0.3019 during averaging but settled at 0.291; test AUPRC 0.437. Console logs confirmed IoU≈0.23 for the best SWA snapshot. | `launch_aggressive_swa.sh`, `configs/experiment/fusion_transformer_aggressive_oct17.yaml` | 0.302 | 0.437 (SWA still trails the 0.313 snapshot; need later start & shorter window). |
| 2025‑10‑23 – `blend_selective_swa_epoch09_12.log` | Selective SWA blend (50/50) of epoch 09 baseline + finetune epoch 12. Test-only evaluation on blended weights. | `scripts/blend_checkpoints.py`, `blend.sh` | – | **0.445** (best test AUPRC to date; consider weight sweeps & late-start SWA from blended checkpoint). |
| 2025‑10‑23 – `training_aggressive_swa_2025-10-23_20-44-45.log` | Short SWA finetune (start=26, window=6) from the 50/50 blend with manual `pos_weight=45`. Validation peaked at 0.3056 around the SWA start, then slid to 0.274 by epoch 31; test AUPRC climbed to 0.446 with recall 0.61. | `launch_aggressive_swa.sh`, `src/callbacks/imbalance_setup.py`, `configs/experiment/fusion_transformer_aggressive_oct17.yaml` | 0.306 (peak) | 0.446 (final SWA checkpoint; val drift suggests over-weighted positives). |
| 2025‑10‑24 – `training_aggressive_swa_2025-10-24_16-28-04.log` | Short SWA rerun with hard-positive replay (`train_hard_pos.json`, repeat = 3) and manual `pos_weight=45`. Validation briefly hit **0.3126** at epoch 26 but averaged down to 0.2796 by epoch 31; test AUPRC stayed at 0.446 with recall 0.61 while attention entropy increased to 0.52. | `launch_aggressive_swa.sh`, `scripts/extract_hard_examples.py`, `src/data/shared_memory_datamodule_v2.py` | **0.313** (peak) | 0.446 (final SWA checkpoint still lags the pre-SWA snapshot; stronger weighting may be required). |
| 2025‑10‑24 – `training_aggressive_swa_2025-10-24_18-15-32.log` | Repeated short SWA with hard-positive replay and manual `pos_weight=50`. Validation topped out at 0.3015 before averaging down to 0.289; test AUPRC remained 0.446 (recall 0.61) but neighbour entropy climbed to ~0.60 and gate α drifted toward 0.65. | `launch_aggressive_swa.sh`, `scripts/extract_hard_examples.py`, `src/data/shared_memory_datamodule_v2.py` | 0.302 (peak) | 0.446 (higher class weight increases attention entropy without improving validation). |

## Recent Structural Changes (Oct 16–17)

1. **Scheduled gate regularisation**  
   Penalty weight, dropout rate, and neighbour-context scale now anneal over the first `gate_schedule_epochs`, with per-epoch logging of `gate_w`, `gate_do`, and `ctx_scale`.

2. **Neighbour-only auxiliary head**  
   Optional lightweight classifier on the neighbour context provides an auxiliary BCE loss (weighted by `aux_neighbor_head_weight`) to keep attention gradients active when the main gate reduces neighbour signal.

3. **Auxiliary scheduling & distance weighting**  
   Auxiliary-head weight now anneals (start/end hyperparameters) and its loss is distance-weighted using neighbour minima so centre residues remain dominant.

4. **SWa finetune launcher**  
   Added `launch_aggressive_swa.sh` to run the aggressive experiment with an SWA finetuning phase (now 60 epochs, optional checkpoint resume).

5. **DDP simplification**  
   Trainer strategy switches to `ddp`, avoiding the extra traversal that was triggered by `find_unused_parameters=True`.

## Next Planned Experiments

1. Evaluate the best pre-SWA snapshot from 2025‑10‑24 (e.g. `logs/fusion_transformer_aggressive/runs/2025-10-24_16-28-21/checkpoints/epoch_26-val_auprc_0.3126.ckpt`) to log its standalone val/test metrics.
2. Blend the epoch‑26 checkpoint with the finetuned/SWA weights (`./blend.sh --inputs <epoch26> <swa> ...`) and sweep ratios to see if the 0.3126 validation peak can be preserved while holding ≥0.445 test AUPRC.
3. If validation still stalls, explore a focal-loss warm-up or partial bias schedule before SWA, reusing the mined hard-positive list, or alternate cycles with/without replay instead of a simple repeat factor.
4. Design a looped training schedule (e.g. every 5 epochs switch the datamodule to hard-positive replay with repeat = 3–4, then revert) before kicking off SWA, to expose the model to difficult positives without saturating α.
